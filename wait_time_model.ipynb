{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5cb440",
   "metadata": {},
   "source": [
    "# Predicting Student Wait Time at LAIR for Stanfordâ€™s Introductory Computer Science Courses\n",
    "\n",
    "## Project Group:\n",
    "Sachin Allums (sachino)\\\n",
    "Justin Blumencranz (jmb25)\\\n",
    "Andrew Hong (amhong)\\\n",
    "Mahathi Mangipudi (mahathim)\n",
    "\n",
    "Stanford enrolls over 2500 students each year in its two introductory computer science courses: CS106A and CS106B. These students have the opportunity to make use of LaIR, a space where they can receive one-on-one help from a section leader with their code for a given assignment. Currently, section leaders of the course are recommended to spend 15 minutes on each help request to better manage the flow of assistance. The purpose of our project is to develop a model that can predict how long students have to wait to receive help based on the assignment they are completing, the time they go to LaIR, and the number of days they go before the assignment deadline, among other features. \n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We have chosen to implement a linear regression model, which will take in a variety of features describing the context of a single LaIR request and output an estimated wait time for the student to recieve help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e3324",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665b5fc",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "Here we import all of the libraries/packages we will need to train and analyze our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784f4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02976ab-8a2c-416d-80c8-2275ee2285ae",
   "metadata": {},
   "source": [
    "# Model Control Panel\n",
    "Use the cell below to tweak the model for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fa34b0-6523-4a2a-8999-aad24c1b657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath to Dataset\n",
    "FILEPATH = \"master_database_March6_forModeling - master_database_March6 (1).csv\"\n",
    "\n",
    "# Define split sizes\n",
    "TRAIN_SIZE = 0.8\n",
    "CV_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "LEARNING_RATE = 5.0e-3\n",
    "ITERATIONS = 100000\n",
    "LAMBDA = None\n",
    "B_INIT = 0.01\n",
    "W_INIT = lambda n_features: np.random.uniform(-1, 1, n_features)\n",
    "\n",
    "# Which Columns are Features\n",
    "X_LABELS = [\n",
    "    \"numInQueue\",\n",
    "    \"daysLeftClean\",\n",
    "    \"bitAndKarel\",\n",
    "    \"imagesAndGraphics\",\n",
    "    \"mapsAndDictionaries\",\n",
    "    \"lambdas\",\n",
    "    \"fileReading\",\n",
    "    \"grids\",\n",
    "    \"strings\",\n",
    "    \"userInteraction\",\n",
    "    \"queuesAndStacks\",\n",
    "    \"recursion\",\n",
    "    \"structs\",\n",
    "    \"objectOrientedProgramming\",\n",
    "    \"pointersAndMemory\",\n",
    "    \"sorting\",\n",
    "    \"hashTables\"\n",
    "]\n",
    "\n",
    "# Which Column is the label\n",
    "Y_LABELS = \"waitTime\"\n",
    "\n",
    "\n",
    "### Don't Touch Anything Below This Line ##################################\n",
    "assert TRAIN_SIZE + CV_SIZE + TEST_SIZE == 1 # Ensure splits add up to 100%\n",
    "assert TRAIN_SIZE > 0\n",
    "assert CV_SIZE > 0\n",
    "assert TEST_SIZE > 0\n",
    "assert len(X_LABELS) > 0\n",
    "assert LEARNING_RATE > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3a860",
   "metadata": {},
   "source": [
    "# Pre-Training Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf9cf7",
   "metadata": {},
   "source": [
    "Below are the neccesary functions to load in the data from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42dd574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(filepath, x_labels, y_labels, training_only=False):\n",
    "    # Load dataset from filepath\n",
    "    filepath = 'master_database_March6_forModeling - master_database_March6 (1).csv'\n",
    "    dtype = {\"waitTime\": int, \"daysLeftClean\": float, \"numInQueue\": float}\n",
    "    dataset = pd.read_csv(filepath, dtype=dtype)\n",
    "\n",
    "    # Split the data\n",
    "    train, test = train_test_split(dataset, test_size=1-TRAIN_SIZE)\n",
    "    test, cv = train_test_split(test, test_size=TEST_SIZE/(1-TRAIN_SIZE))\n",
    "\n",
    "    # Print Split Sizes\n",
    "    print(f\"Total number of examples: {len(dataset)}\")\n",
    "    print(f\"Sizes of TRAIN, CV, TEST: [{len(train)},{len(cv)},{len(test)}]\")\n",
    "    \n",
    "    # Define Dictionaries\n",
    "    TRAIN = {\n",
    "        'X' : np.array(train[x_labels]),\n",
    "        'Y' : np.array(train[y_labels])\n",
    "    }\n",
    "    \n",
    "    CV = {\n",
    "        'X' : np.array(cv[x_labels]),\n",
    "        'Y' : np.array(cv[y_labels])\n",
    "    }\n",
    "    \n",
    "    TEST = {\n",
    "        'X' : np.array(test[x_labels]),\n",
    "        'Y' : np.array(test[y_labels])\n",
    "    }\n",
    "    \n",
    "    # Print Split Shapes\n",
    "    print(f\"X TRAIN Shape: {TRAIN['X'].shape}, X Type:{type(TRAIN['X'])})\")\n",
    "    print(f\"y TRAIN Shape: {TRAIN['Y'].shape}, y Type:{type(TRAIN['Y'])})\")\n",
    "    print(f\"X CV Shape: {CV['X'].shape}, X CV Type:{type(CV['X'])})\")\n",
    "    print(f\"y CV Shape: {CV['Y'].shape}, y CV Type:{type(CV['Y'])})\")\n",
    "    print(f\"X TEST Shape: {TEST['X'].shape}, X CV Type:{type(CV['X'])})\")\n",
    "    print(f\"y TEST Shape: {TEST['Y'].shape}, y CV Type:{type(TEST['Y'])})\")\n",
    "    \n",
    "    if training_only: return TRAIN\n",
    "    else: return TRAIN, CV, TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a6e71-6cf0-4003-a369-afdf43647a31",
   "metadata": {},
   "source": [
    "# Training Utility Functions: Linear Regression\n",
    "Below are the functions used to train the model using gradient decent with MSE cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26d1100-812d-4407-b5a1-cc445485ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    f_wb = (X @ w) + b\n",
    "    error = (f_wb - y)\n",
    "    return (error @ error) / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a88f93-17b5-4529-bfbb-cc21edd45daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_regularized(X, y, w, b, lambda_=0): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    f_wb = (X @ w) + b\n",
    "    error = (f_wb - y)\n",
    "    regularization = (lambda_ / m) * (w @ w)\n",
    "    return (error @ error) / (2 * m) + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1682130e-8e7d-4014-a0ca-3081bfc163e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    error = (X @ w) + b - y\n",
    "    dj_dw = (np.transpose(X) @ error) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "    return dj_db, dj_dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5212bf04-8bd5-48c3-b807-c63e2d554979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_regularized(X, y, w, b, lambda_=0): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    error = (X @ w) + b - y\n",
    "    dj_dw = ((np.transpose(X) @ error) + (lambda_ * w)) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "    return dj_db, dj_dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9efd65a-d686-4e7a-b88a-5c95af7a6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dca10b7-740b-41ad-bcdf-3cb4c687c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    return np.dot(x, w) + b      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a2def3-92d1-4faf-bb86-bad3cf0f3310",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_linear_regression(X_train, Y_train, return_history=True):\n",
    "    m, n = X_train.shape\n",
    "\n",
    "    # Step 1: Initialize Parameters\n",
    "    w = W_INIT(n)                          # Calls lambda W_INIT to populate w with n weights\n",
    "    b = B_INIT                             # Scalar quantity\n",
    "    iterations = ITERATIONS\n",
    "    alpha = LEARNING_RATE\n",
    "\n",
    "    # Step 2: Show Cost Pre-Training\n",
    "    initial_cost = compute_cost(X_train, Y_train, w, b)\n",
    "    print(f\"Initial Cost: {initial_cost}\")\n",
    "\n",
    "\n",
    "    # Step 3: Show Gradiant Pre-Training\n",
    "    tmp_dj_db, tmp_dj_dw = compute_gradient_regularized(X_train, Y_train, w, b)\n",
    "    print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "    print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n",
    "\n",
    "    # Step 4: Run Gradient Decent\n",
    "    w_final, b_final, J_hist = gradient_descent(X_train, Y_train, w, b,\n",
    "                                                    compute_cost_regularized, compute_gradient_regularized, \n",
    "                                                    alpha, iterations)\n",
    "    print(f\"w, b found by gradient descent:\\nw= {w_final}\\nb= {b_final:0.2f}\")\n",
    "\n",
    "\n",
    "    # Step 5: Print a few predictions\n",
    "    num_shown_predictions = 5\n",
    "    for i in range(num_shown_predictions):\n",
    "        print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {Y_train[i]}\")\n",
    "    \n",
    "    # Step 6: Return trained weights and bias\n",
    "    if return_history: return w_final, b_final, J_hist\n",
    "    else: return w_final, b_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac452233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD Gradient Decent\n",
    "\n",
    "# b_init = 0.01\n",
    "# w_init = np.ones(len(X_LABELS))\n",
    "\n",
    "# print(f\"Type of w: {type(w_init)}, and type of b: {type(b_init)}\")\n",
    "\n",
    "# # # Compute and display cost using our pre-chosen optimal parameters. \n",
    "# # cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "# # print(f'Cost at optimal w : {cost}')\n",
    "\n",
    "# #Compute and display gradient \n",
    "# tmp_dj_db, tmp_dj_dw = compute_gradient_regularized(X_train, y_train, w_init, b_init)\n",
    "# print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "# print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n",
    "\n",
    "# # initialize parameters\n",
    "# initial_w = np.zeros_like(w_init)\n",
    "# initial_b = 0.01\n",
    "# iterations = ITERATIONS\n",
    "# alpha = LEARNING_RATE\n",
    "\n",
    "# # run gradient descent \n",
    "# w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "#                                                 compute_cost_regularized, compute_gradient_regularized, \n",
    "#                                                 alpha, iterations)\n",
    "# print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "# m,_ = X_train.shape\n",
    "# for i in range(5):\n",
    "#     print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")\n",
    "\n",
    "# # plot cost versus iteration  \n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "# ax1.plot(J_hist)\n",
    "# ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "# ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "# ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "# ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "# plt.show()\n",
    "\n",
    "# # # Plot for just numInQueue against Wait Time\n",
    "# # plt.scatter(X_train, y_train)\n",
    "# # plt.xlabel('Num in Queue')\n",
    "# # plt.ylabel('Wait Time')\n",
    "# # plt.title(\"Num In Queue vs Wait Time\")\n",
    "# # plt.axline((0, b_final), slope=w_final, linewidth=4, color='r')\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef227b4",
   "metadata": {},
   "source": [
    "# Training Utility Functions: Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8250c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X_train, Y_train):\n",
    "    # Step 1: Define the number of features and output dimension\n",
    "    n_features = X_train.shape[0]\n",
    "    output_dim = 1  \n",
    "\n",
    "    # Define learning rate\n",
    "    alpha = LEARNING_RATE\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = tf.keras.Sequential([\n",
    "        # Add normalization layer\n",
    "        LayerNormalization(axis=1),\n",
    "        # Add first hidden layer with L2 regularization and ReLU activation\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        # Add second hidden layer with L2 regularization and ReLU activation\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        # Add output layer\n",
    "        tf.keras.layers.Dense(output_dim)\n",
    "    ])\n",
    "\n",
    "    # Create Adam optimizer with custom learning rate\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=alpha)\n",
    "\n",
    "    # Compile the model with the custom optimizer\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train, Y_train, epochs=1, batch_size=32)\n",
    "\n",
    "    # Print the model summary\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1c135",
   "metadata": {},
   "source": [
    "# Analysis Utility Functions\n",
    "Below are functions used to analyze the performance of the model on the split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3b1c3ea-c8c9-41a4-838d-76248d3d7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, Y, w, b, threshold):\n",
    "    results = zip(predict(X, w, b), Y)\n",
    "\n",
    "    sum_errors = 0\n",
    "    total_errors = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for x, y in results:\n",
    "        error = abs(x - y)\n",
    "        sum_errors += error\n",
    "        total_errors += 1\n",
    "        if error < threshold:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    print(f\"Accuracy on {label}: Within Threshold={correct} | Not Within Threshold={incorrect} | Percent={(correct / (correct + incorrect)):0.5f} | Average Error={(sum_errors/total_errors):0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8923c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_bins(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    f_wb = (X @ w) + b\n",
    "    diff = abs(f_wb - y)\n",
    "    \n",
    "    small = diff[diff <= 2]\n",
    "    medium = diff[(diff > 2) & (diff < 10)]\n",
    "    large = diff[diff >= 10]\n",
    "    \n",
    "    return len(small), len(medium), len(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44a33a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline_error(threshold):\n",
    "    in_queue_TRAIN = np.array(train[\"numInQueue\"])\n",
    "    Y_train = np.array(train[Y_LABELS])\n",
    "    in_queue_CV = np.array(cv[\"numInQueue\"])\n",
    "    Y_cross = np.array(cv[Y_LABELS])\n",
    "    in_queue_TEST = np.array(test[\"numInQueue\"])\n",
    "    Y_test = np.array(test[Y_LABELS])\n",
    "    \n",
    "    splits = [(in_queue_TRAIN, Y_train, \"TRAIN\"), (in_queue_CV, Y_cross, \"CV\"), (in_queue_TEST, Y_test, \"TEST\")]\n",
    "    \n",
    "    for in_queue, Y, label in splits:\n",
    "        in_queue *= 15 # 15 Minutes Per Person In Queue\n",
    "        results = zip(in_queue, Y)\n",
    "\n",
    "        sum_errors = 0\n",
    "        total_errors = 0\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        \n",
    "        for x, y in results:\n",
    "            error = abs(x - y)\n",
    "            sum_errors += error\n",
    "            total_errors += 1\n",
    "            if error < threshold:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "        print(f\"Accuracy on {label}: Within Threshold={correct} | Not Within Threshold={incorrect} | Percent={(correct / (correct + incorrect)):0.5f} | Average Error={(sum_errors/total_errors):0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c2b99",
   "metadata": {},
   "source": [
    "# Training\n",
    "Here we will load the dataset and train the model by calling our utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f941a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/dhf9b4150274w8097wk48y0w0000gn/T/ipykernel_95145/1409751676.py:5: DtypeWarning: Columns (22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(filepath, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples: 20237\n",
      "Sizes of TRAIN, CV, TEST: [16189,2025,2023]\n",
      "X TRAIN Shape: (16189, 17), X Type:<class 'numpy.ndarray'>)\n",
      "y TRAIN Shape: (16189,), y Type:<class 'numpy.ndarray'>)\n",
      "X CV Shape: (2025, 17), X CV Type:<class 'numpy.ndarray'>)\n",
      "y CV Shape: (2025,), y CV Type:<class 'numpy.ndarray'>)\n",
      "X TEST Shape: (2023, 17), X CV Type:<class 'numpy.ndarray'>)\n",
      "y TEST Shape: (2023,), y CV Type:<class 'numpy.ndarray'>)\n",
      "Initial Cost: 172.62567396067044\n",
      "dj_db at initial w,b: -10.122015954235389\n",
      "dj_dw at initial w,b: \n",
      " [-200.138   -9.566   -0.148   -2.923   -4.38    -0.394   -1.229   -2.342\n",
      "   -0.7     -2.214   -2.12    -2.857   -3.97    -0.703   -1.644   -1.439\n",
      "   -0.079]\n",
      "Iteration    0: Cost    97.59   \n",
      "Iteration 10000: Cost    83.95   \n",
      "Iteration 20000: Cost    83.88   \n",
      "Iteration 30000: Cost    83.87   \n",
      "Iteration 40000: Cost    83.86   \n",
      "Iteration 50000: Cost    83.86   \n",
      "Iteration 60000: Cost    83.86   \n",
      "Iteration 70000: Cost    83.85   \n",
      "Iteration 80000: Cost    83.85   \n",
      "Iteration 90000: Cost    83.85   \n",
      "w, b found by gradient descent:\n",
      "w= [ 1.243 -0.002 -1.955 -3.194  3.998  1.291 -4.205  3.048 -2.551  0.171\n",
      " -5.439 -1.557 -0.025  3.352 -0.866  2.825 -0.061]\n",
      "b= 5.66\n",
      "prediction: 23.56, target value: 16\n",
      "prediction: 14.92, target value: 13\n",
      "prediction: 3.71, target value: 12\n",
      "prediction: 5.52, target value: 1\n",
      "prediction: 9.39, target value: 45\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Dataset\n",
    "TRAIN, CV, TEST = split_dataset(FILEPATH, X_LABELS, Y_LABELS)\n",
    "\n",
    "X_train = TRAIN['X']\n",
    "Y_train = TRAIN['Y']\n",
    "\n",
    "# model = train_neural_network(X_train, Y_train)\n",
    "w, b, J_hist = train_linear_regression(X_train, Y_train, return_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4affc6",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8bdeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 393us/step - loss: 261.3916 - mae: 10.5737\n",
      "Test Loss: 261.3916015625\n",
      "Test Accuracy: 10.573713302612305\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Evaluate\n",
    "X_CV = CV['X']\n",
    "Y_CV = CV['Y'] \n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_train, Y_train)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b42bc5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compute_baseline_error(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m, in \u001b[0;36mcompute_baseline_error\u001b[0;34m(threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_baseline_error\u001b[39m(threshold):\n\u001b[0;32m----> 2\u001b[0m     in_queue_TRAIN \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumInQueue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m     Y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train[Y_LABELS])\n\u001b[1;32m      4\u001b[0m     in_queue_CV \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cv[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumInQueue\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "compute_baseline_error(threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b214f768",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compute_accuracy(X_train, Y_train, w, b, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(X, Y, w, b, threshold)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         incorrect \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Within Threshold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Not Within Threshold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mincorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Percent=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(correct\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(correct\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mincorrect))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Average Error=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(sum_errors\u001b[38;5;241m/\u001b[39mtotal_errors)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "compute_accuracy(X_train, Y_train, w, b, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde64cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost function for cross validation set\n",
    "print(compute_cost_regularized(X_cross, Y_cross, w_final, b_final))\n",
    "small, medium, large = compute_error_bins(X_cross, Y_cross, w_final, b_final)\n",
    "print(f\"small error: {small}, medium error: {medium}, and large error: {large}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs129)",
   "language": "python",
   "name": "cs129"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
