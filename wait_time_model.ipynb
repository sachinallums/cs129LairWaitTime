{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70089695",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Predicting Student Wait Time at LAIR for Stanfordâ€™s Introductory Computer Science Courses\n",
    "\n",
    "## Project Group:\n",
    "Sachin Allums (sachino)\\\n",
    "Justin Blumencranz (jmb25)\\\n",
    "Andrew Hong (amhong)\\\n",
    "Mahathi Mangipudi (mahathim)\n",
    "\n",
    "Stanford enrolls over 2500 students each year in its two introductory computer science courses: CS106A and CS106B. These students have the opportunity to make use of LaIR, a space where they can receive one-on-one help from a section leader with their code for a given assignment. Currently, section leaders of the course are recommended to spend 15 minutes on each help request to better manage the flow of assistance. The purpose of our project is to develop a model that can predict how long students have to wait to receive help based on the assignment they are completing, the time they go to LaIR, and the number of days they go before the assignment deadline, among other features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cb440",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "We have chosen to implement a linear regression model, which will take in a variety of features describing the context of a single LaIR request and output an estimated wait time for the student to recieve help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e3324",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fa116",
   "metadata": {},
   "source": [
    "First, let's import all of the neccesary packages for modeling and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784f4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy, math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02976ab-8a2c-416d-80c8-2275ee2285ae",
   "metadata": {},
   "source": [
    "### Model Control Panel\n",
    "Use the cell below to tweak the model for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa34b0-6523-4a2a-8999-aad24c1b657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split sizes\n",
    "TRAIN_SIZE = 0.8\n",
    "CV_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "LEARNING_RATE = None\n",
    "ITERATIONS = 5.0e-3\n",
    "LAMBDA =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeacab1b",
   "metadata": {},
   "source": [
    "Now we go get the data from our dataset and split it into TRAIN, VAL, and TEST sets! For this we use the handy `train_test_split` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42dd574e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples: 20237\n",
      "Sizes of TRAIN, CV, TEST: [16189,2025,2023]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/dhf9b4150274w8097wk48y0w0000gn/T/ipykernel_42410/3169228000.py:3: DtypeWarning: Columns (22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv('master_database_March6_forModeling - master_database_March6 (1).csv', dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "# File path to the dataset\n",
    "dtype = {\"waitTime\": int, \"daysLeftClean\": float, \"numInQueue\": float}\n",
    "dataset = pd.read_csv('master_database_March6_forModeling - master_database_March6 (1).csv', dtype=dtype)\n",
    "\n",
    "# Ensure splits add up to 100%\n",
    "assert train_size + cv_size + test_size == 1\n",
    "\n",
    "# Split the data\n",
    "train, test = train_test_split(dataset, test_size=1-TRAIN_SIZE)\n",
    "test, crossValidation = train_test_split(test, test_size=TEST_SIZE/(1-TRAIN_SIZE))\n",
    "\n",
    "# Print Set Sizes\n",
    "print(f\"Total number of examples: {len(dataset)}\")\n",
    "print(f\"Sizes of TRAIN, CV, TEST: [{len(train)},{len(crossValidation)},{len(test)}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115bb6b-92ea-42e5-a6ca-347abc37b863",
   "metadata": {},
   "source": [
    "Next we read in the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b7b46f-2ec8-4089-a715-7397de92cc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (16189, 2), X Type:<class 'numpy.ndarray'>)\n",
      "y Shape: (16189,), y Type:<class 'numpy.ndarray'>)\n",
      "X CV Shape: (2025, 2), X CV Type:<class 'numpy.ndarray'>)\n",
      "y CV Shape: (2025, 1), y CV Type:<class 'numpy.ndarray'>)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(train[[\"numInQueue\", \"daysLeftClean\"]])\n",
    "y_train = np.array(train['waitTime'])\n",
    "X_cross = np.array(crossValidation[[\"numInQueue\", \"daysLeftClean\"]])\n",
    "Y_cross = np.array(crossValidation[[\"waitTime\"]])\n",
    "X_test = np.array(test[[\"numInQueue\", \"daysLeftClean\"]])\n",
    "y_test = np.array(test['waitTime'])\n",
    "\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(f\"X CV Shape: {X_cross.shape}, X CV Type:{type(X_train)})\")\n",
    "print(f\"y CV Shape: {Y_cross.shape}, y CV Type:{type(y_train)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec8864a-304f-4b57-b937-0b3d47ae864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_bins(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    small = 0\n",
    "    medium = 0\n",
    "    large = 0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        diff = abs(f_wb_i - y[i])\n",
    "        if diff <= 2:\n",
    "            small += 1\n",
    "        elif diff >= 10:\n",
    "            large += 1    \n",
    "        else:\n",
    "            medium += 1   \n",
    "    return small, medium, large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dca10b7-740b-41ad-bcdf-3cb4c687c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b     \n",
    "    return p    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26d1100-812d-4407-b5a1-cc445485ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):                                \n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1682130e-8e7d-4014-a0ca-3081bfc163e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]   \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]    \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m                                \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9efd65a-d686-4e7a-b88a-5c95af7a6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a2def3-92d1-4faf-bb86-bad3cf0f3310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of w: <class 'numpy.ndarray'>, and type of b: <class 'float'>\n",
      "dj_db at initial w,b: -5.892288591018763\n",
      "dj_dw at initial w,b: \n",
      " [-96.153  -1.735]\n",
      "Iteration    0: Cost   112.61   \n",
      "Iteration  100: Cost    94.08   \n",
      "Iteration  200: Cost    92.74   \n",
      "Iteration  300: Cost    91.91   \n",
      "Iteration  400: Cost    91.41   \n",
      "Iteration  500: Cost    91.11   \n",
      "Iteration  600: Cost    90.92   \n",
      "Iteration  700: Cost    90.81   \n",
      "Iteration  800: Cost    90.74   \n",
      "Iteration  900: Cost    90.70   \n",
      "b,w found by gradient descent: 4.48,[ 1.263 -0.019] \n",
      "prediction: 9.43, target value: 21\n",
      "prediction: 14.59, target value: 12\n",
      "prediction: 17.10, target value: 9\n",
      "prediction: 7.00, target value: 5\n",
      "prediction: 8.16, target value: 7\n",
      "[72.092]\n",
      "small error: 331, medium error: 1238, and large error: 456\n"
     ]
    }
   ],
   "source": [
    "b_init = 0.01\n",
    "w_init = np.array([1, 1])\n",
    "\n",
    "print(f\"Type of w: {type(w_init)}, and type of b: {type(b_init)}\")\n",
    "\n",
    "# # Compute and display cost using our pre-chosen optimal parameters. \n",
    "# cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "# print(f'Cost at optimal w : {cost}')\n",
    "\n",
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n",
    "\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.01\n",
    "iterations = ITERATIONS\n",
    "alpha = LEARNING_RATE\n",
    "\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                compute_cost, compute_gradient, \n",
    "                                                alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(5):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")\n",
    "\n",
    "# # plot cost versus iteration  \n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "# ax1.plot(J_hist)\n",
    "# ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "# ax1.set_title(\"Cost vs. iteration\");  ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "# ax1.set_ylabel('Cost')             ;  ax2.set_ylabel('Cost') \n",
    "# ax1.set_xlabel('iteration step')   ;  ax2.set_xlabel('iteration step') \n",
    "# plt.show()\n",
    "\n",
    "# Compute cost function for cross validation set\n",
    "print(compute_cost(X_cross, Y_cross, w_final, b_final))\n",
    "small, medium, large = compute_error_bins(X_cross, Y_cross, w_final, b_final)\n",
    "print(f\"small error: {small}, medium error: {medium}, and large error: {large}\")\n",
    "\n",
    "# # Plot for just numInQueue against Wait Time\n",
    "# plt.scatter(X_train, y_train)\n",
    "# plt.xlabel('Num in Queue')\n",
    "# plt.ylabel('Wait Time')\n",
    "# plt.title(\"Num In Queue vs Wait Time\")\n",
    "# plt.axline((0, b_final), slope=w_final, linewidth=4, color='r')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabba6da-e9e1-4c1c-b08f-2479482a5978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs129)",
   "language": "python",
   "name": "cs129"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
