{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5cb440",
   "metadata": {},
   "source": [
    "# Predicting Student Wait Time at LAIR for Stanfordâ€™s Introductory Computer Science Courses\n",
    "\n",
    "## Project Group:\n",
    "Sachin Allums (sachino)\\\n",
    "Justin Blumencranz (jmb25)\\\n",
    "Andrew Hong (amhong)\\\n",
    "Mahathi Mangipudi (mahathim)\n",
    "\n",
    "Stanford enrolls over 2500 students each year in its two introductory computer science courses: CS106A and CS106B. These students have the opportunity to make use of LaIR, a space where they can receive one-on-one help from a section leader with their code for a given assignment. Currently, section leaders of the course are recommended to spend 15 minutes on each help request to better manage the flow of assistance. The purpose of our project is to develop a model that can predict how long students have to wait to receive help based on the assignment they are completing, the time they go to LaIR, and the number of days they go before the assignment deadline, among other features. \n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We have chosen to implement a linear regression model, which will take in a variety of features describing the context of a single LaIR request and output an estimated wait time for the student to recieve help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e3324",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665b5fc",
   "metadata": {},
   "source": [
    "# Import Packages\n",
    "Here we import all of the libraries/packages we will need to train and analyze our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784f4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras.layers import LayerNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy, math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make NumPy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02976ab-8a2c-416d-80c8-2275ee2285ae",
   "metadata": {},
   "source": [
    "# Model Control Panel\n",
    "Use the cell below to tweak the model for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4fa34b0-6523-4a2a-8999-aad24c1b657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath to Dataset\n",
    "FILEPATH = \"master_database_March6_forModeling - master_database_March6 (1).csv\"\n",
    "\n",
    "# Define split sizes\n",
    "TRAIN_SIZE = 0.8\n",
    "CV_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "# Model hyperparameters\n",
    "LEARNING_RATE = 5.0e-3\n",
    "ITERATIONS = 100000\n",
    "LAMBDA = 4                                                       # 0 For no regularization\n",
    "B_INIT = 0.01\n",
    "W_INIT = lambda n_features: np.random.uniform(-1, 1, n_features)\n",
    "\n",
    "# Which Columns are Features\n",
    "X_LABELS = [\n",
    "    \"numInQueue\",\n",
    "    \"daysLeftClean\",\n",
    "    \"bitAndKarel\",\n",
    "    \"imagesAndGraphics\",\n",
    "    \"mapsAndDictionaries\",\n",
    "    \"lambdas\",\n",
    "    \"fileReading\",\n",
    "    \"grids\",\n",
    "    \"strings\",\n",
    "    \"userInteraction\",\n",
    "    \"queuesAndStacks\",\n",
    "    \"recursion\",\n",
    "    \"structs\",\n",
    "    \"objectOrientedProgramming\",\n",
    "    \"pointersAndMemory\",\n",
    "    \"sorting\",\n",
    "    \"hashTables\"\n",
    "]\n",
    "\n",
    "# Which Column is the label\n",
    "Y_LABELS = \"waitTime\"\n",
    "\n",
    "\n",
    "### Don't Touch Anything Below This Line ##################################\n",
    "assert TRAIN_SIZE + CV_SIZE + TEST_SIZE == 1 # Ensure splits add up to 100%\n",
    "assert TRAIN_SIZE > 0\n",
    "assert CV_SIZE > 0\n",
    "assert TEST_SIZE > 0\n",
    "assert len(X_LABELS) > 0\n",
    "assert LEARNING_RATE > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3a860",
   "metadata": {},
   "source": [
    "# Pre-Training Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf9cf7",
   "metadata": {},
   "source": [
    "Below are the neccesary functions to load in the data from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42dd574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(filepath, x_labels, y_labels, training_only=False, normalization=True):\n",
    "    # Load dataset from filepath\n",
    "    dtype = {\"waitTime\": int, \"daysLeftClean\": float, \"numInQueue\": float}\n",
    "    dataset = pd.read_csv(filepath, dtype=dtype)\n",
    "\n",
    "    # Split the data\n",
    "    train, test = train_test_split(dataset, test_size=1-TRAIN_SIZE)\n",
    "    test, cv = train_test_split(test, test_size=TEST_SIZE/(1-TRAIN_SIZE))\n",
    "\n",
    "    # Print Split Sizes\n",
    "    print(f\"Total number of examples: {len(dataset)}\")\n",
    "    print(f\"Sizes of TRAIN, CV, TEST: [{len(train)},{len(cv)},{len(test)}]\")\n",
    "    \n",
    "    # Define Dictionaries\n",
    "    TRAIN = {\n",
    "        'X' : np.array(train[x_labels]),\n",
    "        'Y' : np.array(train[y_labels])\n",
    "    }\n",
    "    \n",
    "    CV = {\n",
    "        'X' : np.array(cv[x_labels]),\n",
    "        'Y' : np.array(cv[y_labels])\n",
    "    }\n",
    "    \n",
    "    TEST = {\n",
    "        'X' : np.array(test[x_labels]),\n",
    "        'Y' : np.array(test[y_labels])\n",
    "    }\n",
    "    \n",
    "    # Print Split Shapes\n",
    "#     print(f\"X TRAIN Shape: {TRAIN['X'].shape}, X Type:{type(TRAIN['X'])})\")\n",
    "#     print(f\"y TRAIN Shape: {TRAIN['Y'].shape}, y Type:{type(TRAIN['Y'])})\")\n",
    "#     print(f\"X CV Shape: {CV['X'].shape}, X CV Type:{type(CV['X'])})\")\n",
    "#     print(f\"y CV Shape: {CV['Y'].shape}, y CV Type:{type(CV['Y'])})\")\n",
    "#     print(f\"X TEST Shape: {TEST['X'].shape}, X CV Type:{type(CV['X'])})\")\n",
    "#     print(f\"y TEST Shape: {TEST['Y'].shape}, y CV Type:{type(TEST['Y'])})\")\n",
    "\n",
    "    if normalization:\n",
    "        scaler = preprocessing.StandardScaler().fit(TRAIN['X'])\n",
    "\n",
    "        TRAIN['X'] = scaler.transform(TRAIN['X'])\n",
    "        CV['X'] = scaler.transform(CV['X'])\n",
    "        TEST['X'] = scaler.transform(TEST['X'])\n",
    "    \n",
    "    if training_only: return TRAIN\n",
    "    else: return TRAIN, CV, TEST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a6e71-6cf0-4003-a369-afdf43647a31",
   "metadata": {},
   "source": [
    "# Training Utility Functions: Linear Regression\n",
    "Below are the functions used to train the model using gradient decent with MSE cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a88f93-17b5-4529-bfbb-cc21edd45daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, lambda_=0): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    f_wb = (X @ w) + b\n",
    "    error = (f_wb - y)\n",
    "    regularization = (lambda_ / m) * (w @ w)\n",
    "    return (error @ error) / (2 * m) + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5212bf04-8bd5-48c3-b807-c63e2d554979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=0): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    error = (X @ w) + b - y\n",
    "    dj_dw = ((np.transpose(X) @ error) + (lambda_ * w)) / m\n",
    "    dj_db = np.sum(error) / m\n",
    "    return dj_db, dj_dw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9efd65a-d686-4e7a-b88a-5c95af7a6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(X, y, w, b, lambda_=lambda_))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dca10b7-740b-41ad-bcdf-3cb4c687c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters   \n",
    "      b (scalar):             model parameter \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    return (x @ w) + b      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0a2def3-92d1-4faf-bb86-bad3cf0f3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X_train, Y_train, return_history=True):\n",
    "    m, n = X_train.shape\n",
    "\n",
    "    # Step 1: Initialize Parameters\n",
    "    w = W_INIT(n)                          # Calls lambda W_INIT to populate w with n weights\n",
    "    b = B_INIT                             # Scalar quantity\n",
    "    iterations = ITERATIONS\n",
    "    alpha = LEARNING_RATE\n",
    "    lambda_ = LAMBDA\n",
    "\n",
    "    # Step 2: Show Cost Pre-Training\n",
    "    initial_cost = compute_cost(X_train, Y_train, w, b)\n",
    "    print(f\"Initial Cost: {initial_cost}\")\n",
    "\n",
    "\n",
    "#     # Step 3: Show Gradiant Pre-Training\n",
    "#     tmp_dj_db, tmp_dj_dw = compute_gradient_regularized(X_train, Y_train, w, b)\n",
    "#     print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "#     print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')\n",
    "\n",
    "    # Step 4: Run Gradient Decent\n",
    "    w_final, b_final, J_hist = gradient_descent(X_train, Y_train, w, b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations, lambda_)\n",
    "#     print(f\"w, b found by gradient descent:\\nw= {w_final}\\nb= {b_final:0.2f}\")\n",
    "\n",
    "    print(\"Finished Running Gradient Descent\")\n",
    "\n",
    "    # Step 5: Print a few predictions\n",
    "    num_shown_predictions = 5\n",
    "    for i in range(num_shown_predictions):\n",
    "        print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {Y_train[i]}\")\n",
    "    \n",
    "    # Step 6: Return trained weights and bias\n",
    "    if return_history: return w_final, b_final, J_hist\n",
    "    else: return w_final, b_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c1c135",
   "metadata": {},
   "source": [
    "# Analysis Utility Functions\n",
    "Below are functions used to analyze the performance of the model on the split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3b1c3ea-c8c9-41a4-838d-76248d3d7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X, Y, w, b, threshold):\n",
    "    results = zip(predict(X, w, b), Y)\n",
    "\n",
    "    sum_errors = 0\n",
    "    total_errors = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "\n",
    "    for x, y in results:\n",
    "        error = abs(x - y)\n",
    "        sum_errors += error\n",
    "        total_errors += 1\n",
    "        if error < threshold:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    print(f\"Accuracy on given set: Within Threshold={correct} | Not Within Threshold={incorrect} | Percent={(correct / (correct + incorrect)):0.5f} | Average Error={(sum_errors/total_errors):0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8923c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_bins(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    f_wb = (X @ w) + b\n",
    "    diff = abs(f_wb - y)\n",
    "    \n",
    "    small = diff[diff <= 2]\n",
    "    medium = diff[(diff > 2) & (diff < 10)]\n",
    "    large = diff[diff >= 10]\n",
    "    \n",
    "    return len(small), len(medium), len(large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c2b99",
   "metadata": {},
   "source": [
    "# Training\n",
    "Here we will load the dataset and train the model by calling our utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b248599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples: 20237\n",
      "Sizes of TRAIN, CV, TEST: [16189,2025,2023]\n",
      "[ 0.252 -0.097 -0.166 -0.711 -0.712 -0.276 -0.38  -0.504 -0.338 -0.542\n",
      " -0.544 -0.582  1.519 -0.282  2.489  2.421 -0.093]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/dhf9b4150274w8097wk48y0w0000gn/T/ipykernel_5358/737848859.py:4: DtypeWarning: Columns (22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(filepath, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Dataset\n",
    "TRAIN, CV, TEST = split_dataset(FILEPATH, X_LABELS, Y_LABELS, normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f941a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost: 338.8661143212977\n",
      "Iteration    0: Cost   336.12   \n",
      "Iteration 10000: Cost    82.63   \n",
      "Iteration 20000: Cost    82.62   \n",
      "Iteration 30000: Cost    82.62   \n",
      "Iteration 40000: Cost    82.61   \n",
      "Iteration 50000: Cost    82.61   \n",
      "Iteration 60000: Cost    82.61   \n",
      "Iteration 70000: Cost    82.61   \n",
      "Iteration 80000: Cost    82.61   \n",
      "Iteration 90000: Cost    82.61   \n",
      "Finished Running Gradient Descent\n",
      "prediction: 3.57, target value: 1\n",
      "prediction: 23.53, target value: 16\n",
      "prediction: 17.28, target value: 11\n",
      "prediction: 25.08, target value: 33\n",
      "prediction: 5.30, target value: 1\n"
     ]
    }
   ],
   "source": [
    "# Get training set\n",
    "X_train = TRAIN['X']\n",
    "Y_train = TRAIN['Y']\n",
    "\n",
    "# model = train_neural_network(X_train, Y_train)\n",
    "w, b, J_hist = train_linear_regression(X_train, Y_train, return_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4affc6",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b42bc5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_baseline_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# What would our accuracy be if we just assumed every request takes 15 minutes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m compute_baseline_error(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_baseline_error' is not defined"
     ]
    }
   ],
   "source": [
    "# What would our accuracy be if we just assumed every request takes 15 minutes\n",
    "compute_baseline_error(threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b214f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on given set: Within Threshold=8516 | Not Within Threshold=7673 | Percent=0.52604 | Average Error=8.06380\n",
      "Accuracy on given set: Within Threshold=1043 | Not Within Threshold=980 | Percent=0.51557 | Average Error=8.41328\n"
     ]
    }
   ],
   "source": [
    "# How accurate is our model as predicting wait times within `threshold` minutes\n",
    "compute_accuracy(X_train, Y_train, w, b, threshold=5)\n",
    "compute_accuracy(TEST['X'], TEST['Y'], w, b, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde64cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cost function for cross validation set\n",
    "print(compute_cost_regularized(X_cross, Y_cross, w_final, b_final))\n",
    "small, medium, large = compute_error_bins(X_cross, Y_cross, w_final, b_final)\n",
    "print(f\"small error: {small}, medium error: {medium}, and large error: {large}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743fb5f-eeef-4932-8d2b-f58afc34de2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs129)",
   "language": "python",
   "name": "cs129"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
